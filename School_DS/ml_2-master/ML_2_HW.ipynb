{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\schoolds\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "X = pd.DataFrame(dataset.data)\n",
    "X.columns = dataset.feature_names\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Разделите выборку на обучающую и тестовую в отношении 80%/20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n284   0.00906  90.0   2.97   0.0  0.400  7.088   20.8  7.3073   1.0  285.0   \n285   0.01096  55.0   2.25   0.0  0.389  6.453   31.9  7.3073   1.0  300.0   \n341   0.01301  35.0   1.52   0.0  0.442  7.241   49.3  7.0379   1.0  284.0   \n55    0.01311  90.0   1.22   0.0  0.403  7.249   21.9  8.6966   5.0  226.0   \n..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n414  45.74610   0.0  18.10   0.0  0.693  4.519  100.0  1.6582  24.0  666.0   \n410  51.13580   0.0  18.10   0.0  0.597  5.757  100.0  1.4130  24.0  666.0   \n405  67.92080   0.0  18.10   0.0  0.693  5.683  100.0  1.4254  24.0  666.0   \n418  73.53410   0.0  18.10   0.0  0.679  5.957  100.0  1.8026  24.0  666.0   \n380  88.97620   0.0  18.10   0.0  0.671  6.968   91.9  1.4165  24.0  666.0   \n\n     PTRATIO       B  LSTAT  \n0       15.3  396.90   4.98  \n284     15.3  394.72   7.85  \n285     15.3  394.72   8.23  \n341     15.5  394.74   5.49  \n55      17.9  395.93   4.81  \n..       ...     ...    ...  \n414     20.2   88.27  36.98  \n410     20.2    2.60  10.11  \n405     20.2  384.97  22.98  \n418     20.2   16.45  20.62  \n380     20.2  396.90  17.21  \n\n[404 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n    </tr>\n    <tr>\n      <th>284</th>\n      <td>0.00906</td>\n      <td>90.0</td>\n      <td>2.97</td>\n      <td>0.0</td>\n      <td>0.400</td>\n      <td>7.088</td>\n      <td>20.8</td>\n      <td>7.3073</td>\n      <td>1.0</td>\n      <td>285.0</td>\n      <td>15.3</td>\n      <td>394.72</td>\n      <td>7.85</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>0.01096</td>\n      <td>55.0</td>\n      <td>2.25</td>\n      <td>0.0</td>\n      <td>0.389</td>\n      <td>6.453</td>\n      <td>31.9</td>\n      <td>7.3073</td>\n      <td>1.0</td>\n      <td>300.0</td>\n      <td>15.3</td>\n      <td>394.72</td>\n      <td>8.23</td>\n    </tr>\n    <tr>\n      <th>341</th>\n      <td>0.01301</td>\n      <td>35.0</td>\n      <td>1.52</td>\n      <td>0.0</td>\n      <td>0.442</td>\n      <td>7.241</td>\n      <td>49.3</td>\n      <td>7.0379</td>\n      <td>1.0</td>\n      <td>284.0</td>\n      <td>15.5</td>\n      <td>394.74</td>\n      <td>5.49</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>0.01311</td>\n      <td>90.0</td>\n      <td>1.22</td>\n      <td>0.0</td>\n      <td>0.403</td>\n      <td>7.249</td>\n      <td>21.9</td>\n      <td>8.6966</td>\n      <td>5.0</td>\n      <td>226.0</td>\n      <td>17.9</td>\n      <td>395.93</td>\n      <td>4.81</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>45.74610</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.0</td>\n      <td>0.693</td>\n      <td>4.519</td>\n      <td>100.0</td>\n      <td>1.6582</td>\n      <td>24.0</td>\n      <td>666.0</td>\n      <td>20.2</td>\n      <td>88.27</td>\n      <td>36.98</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>51.13580</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.0</td>\n      <td>0.597</td>\n      <td>5.757</td>\n      <td>100.0</td>\n      <td>1.4130</td>\n      <td>24.0</td>\n      <td>666.0</td>\n      <td>20.2</td>\n      <td>2.60</td>\n      <td>10.11</td>\n    </tr>\n    <tr>\n      <th>405</th>\n      <td>67.92080</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.0</td>\n      <td>0.693</td>\n      <td>5.683</td>\n      <td>100.0</td>\n      <td>1.4254</td>\n      <td>24.0</td>\n      <td>666.0</td>\n      <td>20.2</td>\n      <td>384.97</td>\n      <td>22.98</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>73.53410</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.0</td>\n      <td>0.679</td>\n      <td>5.957</td>\n      <td>100.0</td>\n      <td>1.8026</td>\n      <td>24.0</td>\n      <td>666.0</td>\n      <td>20.2</td>\n      <td>16.45</td>\n      <td>20.62</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>88.97620</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.0</td>\n      <td>0.671</td>\n      <td>6.968</td>\n      <td>91.9</td>\n      <td>1.4165</td>\n      <td>24.0</td>\n      <td>666.0</td>\n      <td>20.2</td>\n      <td>396.90</td>\n      <td>17.21</td>\n    </tr>\n  </tbody>\n</table>\n<p>404 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sort_values(by='CRIM')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Обучите стандартную регрессию, а также Ridge и Lasso и параметрами по умолчанию и выведите их R2 на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение метрики r2 (коэффициент детерминации) для модели линейной регрессии: 0.7064403227878493\n",
      "Значение метрики r2 (коэффициент детерминации) для модели Ridge: 0.7244072760384892\n",
      "Значение метрики r2 (коэффициент детерминации) для модели LASSO: 0.13228963154072448\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model as lm\n",
    "\n",
    "lin_reg = lm.LinearRegression()\n",
    "fit_lin_reg = lin_reg.fit(X_train, y_train)\n",
    "\n",
    "ridge_reg = lm.Ridge(RANDOM_STATE)\n",
    "fit_ridge_reg = ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "lasso_reg = lm.Lasso(RANDOM_STATE)\n",
    "fit_lasso_reg = lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('Значение метрики r2 (коэффициент детерминации) для модели линейной регрессии: ' + str(r2_score(y_test, lin_reg.predict(X_test))))\n",
    "print('Значение метрики r2 (коэффициент детерминации) для модели Ridge: ' + str(r2_score(y_test, ridge_reg.predict(X_test))))\n",
    "print('Значение метрики r2 (коэффициент детерминации) для модели LASSO: ' + str(r2_score(y_test, lasso_reg.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# pd.DataFrame(ridge_reg.coef_, X_train.columns, columns=['coef']).sort_values(by='coef', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Для Ridge и Lasso подберите коэффициент регуляризации(используйте GridSearchCV, RidgeCV, LassoCV) в пределах от $10^{-5}$ до $10^5$ (по степеням 10). Посчитайте R2 на тестовой выборке по лучшим моделям и сравните с предыдущими результатами. Напишите как изменился результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.1)\n",
      "Lasso(alpha=0.0001, random_state=42)\n",
      "Метрика r2 (коэффициент детерминации) по модели Ridge с кросс-валидацией и лучшим коэффициентом регуляризации: \r\n",
      "0.7087090635048612\n",
      "Метрика r2 (коэффициент детерминации) по модели LASSO с кросс-валидацией и лучшим коэффициентом регуляризации: \r\n",
      "0.7064933046514985\n"
     ]
    }
   ],
   "source": [
    "alphas = list(np.geomspace(10**-5, 10**5, 11))\n",
    "\n",
    "grid_search_cv_ridge = (GridSearchCV(lm.Ridge(), {'alpha': alphas}, scoring='r2')).fit(X_train, y_train)\n",
    "grid_search_cv_lasso = (GridSearchCV(lm.Lasso(random_state=RANDOM_STATE), {'alpha': alphas}, scoring='r2')).fit(X_train, y_train)\n",
    "\n",
    "print(grid_search_cv_ridge.best_estimator_)\n",
    "print(grid_search_cv_lasso.best_estimator_)\n",
    "print('Метрика r2 (коэффициент детерминации) по модели Ridge с кросс-валидацией и лучшим коэффициентом регуляризации: \\r\\n'\n",
    "      + str(grid_search_cv_ridge.best_estimator_.score(X=X_test, y=y_test)))\n",
    "print('Метрика r2 (коэффициент детерминации) по модели LASSO с кросс-валидацией и лучшим коэффициентом регуляризации: \\r\\n'\n",
    "      + str(grid_search_cv_lasso.best_estimator_.score(X=X_test, y=y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Вывод\n",
    "Модель по Ridge изменилась незначительно, по большей части уменьшился, вероятно, из-за наличия кросс-валидации\n",
    "Модель по LASSO улучшилась, благодаря подобранному параметру."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Проведите масштабирование выборки(используйте Pipeline, StandardScaler, MinMaxScaler), посчитайте R2 и сравните с предыдущими результатами. Напишите как изменился результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика r2 на масштабированной (MinMaxScaler) выборке по методу Ridge: 0.7070845645507637\r\n",
      "Метрика r2 на масштабированной (MinMaxScaler) выборке по методу Lasso: 0.2533189827893051\r\n",
      "Метрика r2 на масштабированной (StandardScaler) выборке по методу Ridge: 0.7065344206975689\r\n",
      "Метрика r2 на масштабированной (StandardScaler) выборке по методу Lasso: 0.6492888897832787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "scalers = [MinMaxScaler(), StandardScaler()]\n",
    "models = [lm.Ridge(), lm.Lasso(random_state=RANDOM_STATE)]\n",
    "from itertools import product\n",
    "\n",
    "scal_mods = list(product(scalers, models))\n",
    "\n",
    "print(*list((map(lambda x: 'Метрика r2 на масштабированной ('\n",
    "                              + str(x[0]).split('(')[0] + ') выборке по методу '\n",
    "                              + str(x[1]).split('(')[0] + ': '\n",
    "                              + str((Pipeline([('scaler', x[0]), ('model', x[1])]).fit(X_train, y_train)).score(X_test, y_test))\n",
    "                    , scal_mods))), sep='\\r\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Вывод\n",
    "MinMaxScaler отработал лучше при использовании Ridge и лучше в сравнение с StandardScaler.\n",
    "При использовании Lasso лучше масштабирование было не использовать. Метрика только ухудшилась."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Подберите коэффициент регуляризации для Ridge и Lasso на масштабированных данных, посчитайте R2 и сравните с предыдущими результатами. Напишите как изменился результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования MinMaxScaler и модели Ridge: 0.48795625790832453\n",
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования MinMaxScaler и модели Lasso: 0.44364915632256885\n",
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования StandardScaler и модели Ridge: 0.6707509520722196\n",
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования StandardScaler и модели Lasso: 0.669271600528933\n"
     ]
    }
   ],
   "source": [
    "for scaler, model in scal_mods:\n",
    "    X_train_scaled = scaler.fit_transform(X_train, y_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test, y_test)\n",
    "    print('Результат метрики r2 при подборе коэффициента регуляризации для масштабирования '\n",
    "          + str(scaler).split('(')[0]\n",
    "          + ' и модели '\n",
    "          + str(model).split('(')[0]\n",
    "          + ': '\n",
    "          + str((GridSearchCV(model, {'alpha': alphas}, scoring='r2').fit(X_train_scaled, y_train)).best_estimator_.score(X=X_test_scaled, y=y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования MinMaxScaler и модели Ridge: 0.48795625790832453\r\n",
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования MinMaxScaler и модели Lasso: 0.44364915632256885\r\n",
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования StandardScaler и модели Ridge: 0.6707509520722196\r\n",
      "Результат метрики r2 при подборе коэффициента регуляризации для масштабирования StandardScaler и модели Lasso: 0.669271600528933\n"
     ]
    }
   ],
   "source": [
    "print(*list(map(lambda x: 'Результат метрики r2 при подборе коэффициента регуляризации для масштабирования '\n",
    "                          + str(x[0]).split('(')[0]\n",
    "                          + ' и модели '\n",
    "                          + str(x[1]).split('(')[0]\n",
    "                          + ': '\n",
    "                          + str((GridSearchCV(x[1], {'alpha': alphas}, scoring='r2').fit(x[0].fit_transform(X_train, y_train), y_train))\n",
    "                                .best_estimator_\n",
    "                                .score(X=x[0].fit_transform(X_test, y_test), y=y_test)), scal_mods)), sep='\\r\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Вывод\n",
    "При использовании масштабирования везде идет прирост значения метрики r2, кроме случая применения масштабирования MinMaxScaler для модели Ridge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Добавьте попарные произведения признаков и их квадраты (используйте PolynomialFeatures) на масштабированных признаках, посчитайте R2 и сравните с предыдущими результатами. Напишите как изменился результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика r2 на масштабируемой выборке (MinMaxScaler) с применением полиномиальных признаков для модели Ridge: 0.7213499736691886\n",
      "Метрика r2 на масштабируемой выборке (MinMaxScaler) с применением полиномиальных признаков для модели Lasso: 0.2596801657985459\n",
      "Метрика r2 на масштабируемой выборке (StandardScaler) с применением полиномиальных признаков для модели Ridge: 0.4112684529743279\n",
      "Метрика r2 на масштабируемой выборке (StandardScaler) с применением полиномиальных признаков для модели Lasso: 0.7404276060048538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "for scaler, model in scal_mods:\n",
    "    X_train_scaled = scaler.fit_transform(X_train, y_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test, y_test)\n",
    "    X_train_poly = PolynomialFeatures(degree=(1, 2)).fit_transform(X_train_scaled, y_train)\n",
    "    X_test_poly = PolynomialFeatures(degree=(1, 2)).fit_transform(X_test_scaled, y_test)\n",
    "    print('Метрика r2 на масштабируемой выборке ('\n",
    "          + str(scaler).split('(')[0]\n",
    "          + ') с применением полиномиальных признаков для модели '\n",
    "          + str(model).split('(')[0]\n",
    "          + ': '\n",
    "          + str(r2_score(y_test, (model.fit(X_train_poly, y_train)).predict(X_test_poly))))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Вывод:\n",
    "Использование полиномиальных признаков существенно повысило метрику r2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Подберите наилучшую модель (используйте Pipeline, GridSearchSCV) подбирая тип регуляризации (L1,L2), коэффициент регуляризации, метод масштабирования и степень полинома в PolynomialFeatures. Выведите итоговые параметры и результат R2. Напишите как изменился R2 по сравнению с предыдущими экспериментами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from itertools import combinations\n",
    "poly_degree = list(combinations(list(range(5)), 2))\n",
    "\n",
    "scal_mods_poly = list(product(scalers, models, poly_degree))\n",
    "\n",
    "list_params = []\n",
    "for scaler, model, poly in scal_mods_poly:\n",
    "    X_train_scaled = scaler.fit_transform(X_train, y_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test, y_test)\n",
    "    X_train_poly = PolynomialFeatures(degree=poly).fit_transform(X_train_scaled, y_train)\n",
    "    X_test_poly = PolynomialFeatures(degree=poly).fit_transform(X_test_scaled, y_test)\n",
    "    grid_search_be = GridSearchCV(model, {'alpha': alphas}, scoring='r2').fit(X_train_poly, y_train).best_estimator_\n",
    "    list_params.append([scaler, grid_search_be, poly, grid_search_be.score(y=y_test, X=X_test_poly)])\n",
    "list_params\n",
    "df_result = pd.DataFrame(list_params, columns=['scaler', 'model', 'polynomial', 'r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "scaler                             MinMaxScaler()\nmodel         Lasso(alpha=0.001, random_state=42)\npolynomial                                 (3, 4)\nr2                                       0.794119\nName: value, dtype: object"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(df_result[df_result['r2'] == df_result['r2'].max()].iloc[0]).rename('value')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/adult-all.csv'\n",
    "\n",
    "import os\n",
    "import requests as rq\n",
    "\n",
    "if not os.path.exists(os.path.abspath(os.path.curdir) + '\\\\data'):\n",
    "    print('Cоздаем отсутствующую папку для хранения данных')\n",
    "    os.mkdir(os.path.abspath(os.path.curdir) + '\\\\data')\n",
    "if not os.path.exists(os.path.abspath(os.path.curdir) + '\\\\data\\\\adult-all.csv'):\n",
    "    print('Скачиваем файл: ' + link)\n",
    "    with open(os.path.abspath(os.path.curdir) + '\\\\data\\\\adult-all.csv', 'wb') as f:\n",
    "        f.write((rq.get(link)).content)\n",
    "        f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Разделите выборку на признаки и целевую переменную(колонка со зачениями {<=50K,>50K}). Замените целевую переменную на числовые значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   age      company_type       2  education   4             married  \\\n0   39         State-gov   77516  Bachelors  13       Never-married   \n1   50  Self-emp-not-inc   83311  Bachelors  13  Married-civ-spouse   \n2   38           Private  215646    HS-grad   9            Divorced   \n3   53           Private  234721       11th   7  Married-civ-spouse   \n4   28           Private  338409  Bachelors  13  Married-civ-spouse   \n\n            position  family_status skin_color     sex    10  11  12  \\\n0       Adm-clerical  Not-in-family      White    Male  2174   0  40   \n1    Exec-managerial        Husband      White    Male     0   0  13   \n2  Handlers-cleaners  Not-in-family      White    Male     0   0  40   \n3  Handlers-cleaners        Husband      Black    Male     0   0  40   \n4     Prof-specialty           Wife      Black  Female     0   0  40   \n\n         country  \n0  United-States  \n1  United-States  \n2  United-States  \n3  United-States  \n4           Cuba  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>company_type</th>\n      <th>2</th>\n      <th>education</th>\n      <th>4</th>\n      <th>married</th>\n      <th>position</th>\n      <th>family_status</th>\n      <th>skin_color</th>\n      <th>sex</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.abspath(os.path.curdir) + '\\\\data\\\\adult-all.csv', header=None)\n",
    "\n",
    "data['result'] = np.nan\n",
    "data['result'].iloc[data[data[14] == '>50K'].index.to_list()] = 1\n",
    "data['result'].iloc[data[data[14] == '<=50K'].index.to_list()] = 0\n",
    "data = data.rename(columns={0: 'age', 1: 'company_type', 3: 'education', 5: 'married', 6: 'position', 7: 'family_status', 8: 'skin_color', 9: 'sex', 13: 'country', 14: 'salary'})\n",
    "data_X = data.drop(columns=['result', 'salary'])\n",
    "data_y = data[['result']]\n",
    "data_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Выясните, присутствуют ли в данных пропуски. Заполните их самыми частыми значениями (испольуйте SimpleImputer)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "ls_col_nulls = []\n",
    "for key in data_X.keys().tolist():\n",
    "    ls_col_nulls.append([key, data[data_X[key].isna()][key].count()])\n",
    "\n",
    "df_cols_nulls = pd.DataFrame(ls_col_nulls)\n",
    "\n",
    "if df_cols_nulls[1].sum() == 0: print('Пропуски в данных отсутствуют')\n",
    "else:\n",
    "    data_X = pd.DataFrame((SimpleImputer(missing_values=None, strategy='most_frequent').fit_transform(data_X)), columns=data_X.keys().tolist())"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропуски в данных отсутствуют\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Выберите колонки с числовыми и категориальными переменными."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['company_type', 'married', 'family_status', 'skin_color', 'sex']\n",
      "['age', 2, 4, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "ls_cat = []\n",
    "ls_int = []\n",
    "for i in data_X.keys().to_list():\n",
    "    # print(str(i) + ': ' + str(data_X[i].nunique()), end='; ')\n",
    "    if data_X[i].nunique() <= 10: ls_cat.append(i)\n",
    "    if data_X[i].dtype == np.dtype(np.int64): ls_int.append(i)\n",
    "print('')\n",
    "print(ls_cat)\n",
    "print(ls_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Создайте пайплайн по обработке колонок(используйте OneHotEncoder,MinMaxScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data_X_class = pd.merge(left=pd.DataFrame(Pipeline(steps=[('cat', OneHotEncoder())]).fit_transform(data_X[ls_cat]).toarray()).reset_index()\n",
    "                              , right=pd.DataFrame(Pipeline([('scaler', MinMaxScaler())]).fit_transform(data_X[ls_int])).reset_index()\n",
    "                              , on='index', ).set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Посчитайте метрики accuracy и f1_score на предсказании только самого частого класса в целевой переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.3969017193960598\n",
      "accuracy_score: 0.3400147414110806\n"
     ]
    }
   ],
   "source": [
    "ls_bin_cat = []\n",
    "for i in data_X.keys().to_list():\n",
    "    if data_X[i].nunique() == 2: ls_bin_cat.append(i)\n",
    "\n",
    "ls_max_cat = []\n",
    "for i in ls_cat:\n",
    "    value, quantity = (data[[i]].groupby(by=[i])[[i]].count().rename(columns={i: 'quantity'})).sort_values(by='quantity', ascending=False).reset_index().iloc[0].to_list()\n",
    "    ls_max_cat.append([i, value, quantity])\n",
    "\n",
    "ls_max = pd.DataFrame(ls_max_cat).max().to_list()\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print('f1_score: ' + str(f1_score(data_y, (data_X[ls_max[0]] == ls_max[1]).astype(int))))\n",
    "print('accuracy_score: ' + str(accuracy_score(data_y, (data_X[ls_max[0]] == ls_max[1]).astype(int))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Посчитайте cross_val_score по алгоритмам LogisticRegression, SVC, LinearSVC по метрикам accuracy и f1_score.\n",
    "Напишите удалось ли превзойти предыдущий результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "    0.8474767120483161\n",
      "    f1_score: 0.6500704556129638\n",
      "    cross_val_score: [0.8431774  0.84307503 0.84961097 0.84438984 0.84479934]\n",
      "    cross_val_score: 0.845010518991991\n",
      "SVC()\n",
      "    0.8452246903470161\n",
      "    f1_score: 0.6321167883211678\n",
      "    cross_val_score: [0.83979937 0.84225612 0.8482801  0.84152334 0.84520885]\n",
      "    cross_val_score: 0.8434135533275671\n",
      "LinearSVC()\n",
      "    0.8474767120483161\n",
      "    f1_score: 0.6454069490718706\n",
      "    cross_val_score: [0.8431774  0.8463507  0.84817772 0.8458231  0.84531122]\n",
      "    cross_val_score: 0.8457680276801989\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data_X_class_train, data_X_class_test, data_y_class_train, data_y_class_test = train_test_split(data_X_class, data_y, test_size=0.2, train_size=0.8)\n",
    "\n",
    "for model in [LogisticRegression(), SVC(), LinearSVC()]:\n",
    "    print(model)\n",
    "    estim = model.fit(data_X_class_train, data_y_class_train)\n",
    "    print('    ' + str(accuracy_score(data_y_class_test, estim.predict(data_X_class_test))))\n",
    "    print('    f1_score: ' + str(f1_score(data_y_class_test, estim.predict(data_X_class_test))))\n",
    "    svc = cross_val_score(model, data_X_class, data_y)\n",
    "    print('    cross_val_score: ' + str(svc))\n",
    "    print('    cross_val_score: ' + str(svc.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Можно заметить что в данных присутствуют значения '?', замените их самыми частыми значениями (испольуйте SimpleImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_edited = data.copy()\n",
    "for i in data_edited.keys().to_list(): data_edited[i].iloc[data[data[i] == '?'].index.to_list()] = None\n",
    "\n",
    "data_edited = pd.DataFrame((SimpleImputer(missing_values=None, strategy='most_frequent').fit_transform(data_X)), columns=data_X.keys().tolist())\n",
    "\n",
    "data_X_edited = pd.merge(left=pd.DataFrame(Pipeline(steps=[('cat', OneHotEncoder())]).fit_transform(data_edited[ls_cat]).toarray()).reset_index()\n",
    "                              , right=pd.DataFrame(Pipeline([('scaler', MinMaxScaler())]).fit_transform(data_edited[ls_int])).reset_index()\n",
    "                              , on='index', ).set_index('index')\n",
    "\n",
    "for i in data_edited.keys().to_list():\n",
    "    if type(data_edited[i].iloc[0]) != type(''): data_edited[i] = data_edited[i].astype(int)\n",
    "\n",
    "data_X_edited_train, data_X_edited_test, data_y_edited_train, data_y_edited_test = train_test_split(\n",
    "    data_X_edited\n",
    "    , data_y\n",
    "    , test_size=0.2\n",
    "    , train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Посчитайте cross_val_score на новых данных. Напишите удалось ли улучшить результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "    0.8538233186610708\n",
      "    f1_score: 0.6651031894934334\n",
      "    cross_val_score: [0.8431774  0.84307503 0.84961097 0.84438984 0.84479934]\n",
      "    cross_val_score: 0.845010518991991\n",
      "SVC()\n",
      "    0.8478861705394616\n",
      "    f1_score: 0.636852394916911\n",
      "    cross_val_score: [0.83979937 0.84225612 0.8482801  0.84152334 0.84520885]\n",
      "    cross_val_score: 0.8434135533275671\n",
      "LinearSVC()\n",
      "    0.8529020370559934\n",
      "    f1_score: 0.6569587013607067\n",
      "    cross_val_score: [0.8431774  0.8463507  0.8482801  0.8458231  0.84531122]\n",
      "    cross_val_score: 0.845788502700674\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for model in [LogisticRegression(), SVC(), LinearSVC()]:\n",
    "    print(model)\n",
    "    estim = model.fit(data_X_edited_train, data_y_edited_train)\n",
    "    print('    ' + str(accuracy_score(data_y_edited_test, estim.predict(data_X_edited_test))))\n",
    "    print('    f1_score: ' + str(f1_score(data_y_edited_test, estim.predict(data_X_edited_test))))\n",
    "    svc = cross_val_score(model, data_X_edited, data_y)\n",
    "    print('    cross_val_score: ' + str(svc))\n",
    "    print('    cross_val_score: ' + str(svc.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Посчитайте cross_val_score, если просто удалить значения '?'. Напишите как изменился результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "    0.8389165284687673\n",
      "    f1_score: 0.6415744157441574\n",
      "    cross_val_score: [0.8379215  0.83957988 0.84674923 0.83834586 0.8404467 ]\n",
      "    cross_val_score: 0.8406086355289257\n",
      "SVC()\n",
      "    0.8353786622443339\n",
      "    f1_score: 0.6173220251863275\n",
      "    cross_val_score: [0.83637369 0.83869541 0.8449801  0.83569217 0.83945157]\n",
      "    cross_val_score: 0.8390385875917967\n",
      "LinearSVC()\n",
      "    0.8399115533443892\n",
      "    f1_score: 0.6394422310756972\n",
      "    cross_val_score: [0.83803206 0.84344942 0.84509067 0.84111013 0.84066785]\n",
      "    cross_val_score: 0.8416700247350402\n",
      "Wall time: 5min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_drop = data.copy()\n",
    "for i in data_drop.keys().to_list(): data_drop[i].iloc[data[data[i] == '?'].index.to_list()] = None\n",
    "# data_edited[['result']]\n",
    "# data_drop.info()\n",
    "data_dropped = data_drop.dropna(axis=0)\n",
    "\n",
    "data_all_dropped = pd.merge(left=pd.DataFrame(Pipeline(steps=[('cat', OneHotEncoder())]).fit_transform(data_dropped[ls_cat]).toarray()).reset_index()\n",
    "                              , right=pd.DataFrame(Pipeline([('scaler', MinMaxScaler())]).fit_transform(data_dropped[ls_int])).reset_index()\n",
    "                              , on='index', ).set_index('index')\n",
    "\n",
    "data_X_dropped_train, data_X_dropped_test, data_y_dropped_train, data_y_dropped_test = train_test_split(\n",
    "    data_all_dropped\n",
    "    , data_dropped[['result']]\n",
    "    , test_size=0.2\n",
    "    , train_size=0.8)\n",
    "\n",
    "for model in [LogisticRegression(), SVC(), LinearSVC()]:\n",
    "    print(model)\n",
    "    est = model.fit(data_X_dropped_train, data_y_dropped_train)\n",
    "    print('    ' + str(accuracy_score(data_y_dropped_test, est.predict(data_X_dropped_test))))\n",
    "    print('    f1_score: ' + str(f1_score(data_y_dropped_test, est.predict(data_X_dropped_test))))\n",
    "    svc = cross_val_score(model, data_all_dropped, data_dropped[['result']])\n",
    "    print('    cross_val_score: ' + str(svc))\n",
    "    print('    cross_val_score: ' + str(svc.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 17. Посчитайте cross_val_score для RandomForestClassifier,GradientBoostingClassifier. Напишите как изменился результат и какой вывод можно из этого сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(): [0.84716962 0.84440577 0.84981572 0.84561835 0.84377559]; mean = 0.8461570111508692\n",
      "GradientBoostingClassifier(): [0.86160303 0.86242195 0.86762899 0.86302211 0.86506962]; mean = 0.8639491395377361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "for clf in [RandomForestClassifier(), GradientBoostingClassifier()]:\n",
    "    cvs = cross_val_score(clf, data_X_edited, data_y)\n",
    "    print(str(clf) + ': ' + str(cvs) + '; mean = ' + str(cvs.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Подберите наилучшую модель, подбирая методы обработки колонок - масштабирование признаков, кодирование признаков и заполнение пропусков. Параметры алгоритмов оставьте по умолчанию. Выведите итоговые параметры и результат accuracy и f1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "    f1:       0.6270346779900919\n",
      "    accuracy: 0.8381615313747569\n",
      "    cvs:      [0.8431774  0.84307503 0.84961097 0.84438984 0.84479934]\n",
      "    cvs.mean: 0.845010518991991\n",
      "SVC()\n",
      "    f1:       0.6106685976345643\n",
      "    accuracy: 0.8348858634455932\n",
      "    cvs:      [0.83979937 0.84225612 0.8482801  0.84152334 0.84520885]\n",
      "    cvs.mean: 0.8434135533275671\n",
      "LinearSVC()\n",
      "    f1:       0.6296472831267874\n",
      "    accuracy: 0.8409253761899887\n",
      "    cvs:      [0.8431774  0.8463507  0.8482801  0.8458231  0.84531122]\n",
      "    cvs.mean: 0.845788502700674\n",
      "RandomForestClassifier()\n",
      "    f1:       0.6642841188295735\n",
      "    accuracy: 0.8461459719520934\n",
      "    cvs:      [0.84553178 0.84368922 0.8499181  0.8474611  0.84428747]\n",
      "    cvs.mean: 0.8461775343774525\n",
      "GradientBoostingClassifier()\n",
      "    f1:       0.6723809523809524\n",
      "    accuracy: 0.8591462790459617\n",
      "    cvs:      [0.86160303 0.86242195 0.86762899 0.86302211 0.86506962]\n",
      "    cvs.mean: 0.8639491395377361\n",
      "LogisticRegression()\n",
      "    f1:       0.6442658875091307\n",
      "    accuracy: 0.8504452861091207\n",
      "    cvs:      [0.84379159 0.84563415 0.8490991  0.8462326  0.84592547]\n",
      "    cvs.mean: 0.846136580144667\n",
      "SVC()\n",
      "    f1:       0.6481153554262585\n",
      "    accuracy: 0.8576108097041663\n",
      "    cvs:      [0.84921691 0.85146893 0.85595823 0.85042998 0.85432023]\n",
      "    cvs.mean: 0.8522788557362209\n",
      "LinearSVC()\n",
      "    f1:       0.6365889935547844\n",
      "    accuracy: 0.8499334629951889\n",
      "    cvs:      [0.84379159 0.84655543 0.84879197 0.84643735 0.84490172]\n",
      "    cvs.mean: 0.8460956112404571\n",
      "RandomForestClassifier()\n",
      "    f1:       0.6547976878612717\n",
      "    accuracy: 0.847169618179957\n",
      "    cvs:      [0.84829563 0.84645307 0.85002048 0.84449222 0.84398034]\n",
      "    cvs.mean: 0.8466483466688196\n",
      "GradientBoostingClassifier()\n",
      "    f1:       0.6774918810891831\n",
      "    accuracy: 0.8678472719828028\n",
      "    cvs:      [0.86160303 0.86242195 0.86762899 0.86302211 0.86506962]\n",
      "    cvs.mean: 0.8639491395377361\n",
      "Wall time: 14min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X = data_edited.copy()\n",
    "df_y = data_y\n",
    "\n",
    "ls_cat = []\n",
    "ls_int = []\n",
    "for i in X.keys().to_list():\n",
    "    if X[i].nunique() <= 10: ls_cat.append(i)\n",
    "    if X[i].dtype == np.dtype(np.int): ls_int.append(i)\n",
    "\n",
    "models = [LogisticRegression(), SVC(), LinearSVC(), RandomForestClassifier(), GradientBoostingClassifier()]\n",
    "cat = [OneHotEncoder()]\n",
    "scalers = [MinMaxScaler(), StandardScaler()]\n",
    "\n",
    "list_params = []\n",
    "\n",
    "for categ, scaler in list(product(cat, scalers)):\n",
    "    df_X = pd.merge(left=pd.DataFrame(Pipeline(steps=[('cat', categ)]).fit_transform(X[ls_cat]).toarray()).reset_index()\n",
    "                    , right=pd.DataFrame(Pipeline([('scaler', scaler)]).fit_transform(X[ls_int])).reset_index()\n",
    "                    , on='index').set_index('index')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, train_size=0.8)\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        estim = model.fit(X_train, y_train)\n",
    "        accur = accuracy_score(y_test, estim.predict(X_test))\n",
    "        f1 = f1_score(y_test, estim.predict(X_test))\n",
    "        cvs = cross_val_score(model, df_X, df_y)\n",
    "        print('    f1:       ' + str(f1))\n",
    "        print('    accuracy: ' + str(accur))\n",
    "        print('    cvs:      ' + str(cvs))\n",
    "        print('    cvs.mean: ' + str(cvs.mean()))\n",
    "    list_params.append([categ, scaler, model, f1, accur, cvs.mean()])\n",
    "df_res = pd.DataFrame(list_params, columns=['categorial', 'scaler', 'model', 'f1', 'accuracy', 'cvs_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "categorial                                      OneHotEncoder()\nscaler                                           MinMaxScaler()\nmodel         ([DecisionTreeRegressor(criterion='friedman_ms...\nf1                                                     0.672381\naccuracy                                               0.859146\ncvs_mean                                               0.863949\nName: 0, dtype: object"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[df_res['cvs_mean'] == df_res['cvs_mean'].min()].iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}