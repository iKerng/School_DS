{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NLP2-homework",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "text = '<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLP2_1\n",
    "https://www.hackerrank.com/challenges/detect-the-email-addresses/problem?isFullScreen=true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
    "import re\n",
    "\n",
    "result = set(re.findall('[0-9A-z_]{1,}@[0-9A-z_]{1,}\\.[0-9A-z_]{1,}', text))\n",
    "result = list(result)\n",
    "result.sort()\n",
    "print(';'.join(result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLP2_2 https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w3.org\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
    "import re\n",
    "\n",
    "ls = re.findall('https{0,1}://[a-z.0-9]{1,}[.][a-z.0-9]{1,}[/?\"]', text)\n",
    "ls = list(set([re.sub('[!w]{3}[.]|[/?\"]|(https{0,1}:)', '', x) for x in ls]))\n",
    "ls.sort()\n",
    "print(';'.join(ls))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLP2_3 Реализовать классификатор токсичных комментариев tfidf на базе датасета\n",
    "https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments\n",
    "\n",
    "Дубликат файла: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16706093\\PycharmProjects\\School_DS\\NLP-3\\data\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                             comment  toxic\n0               Верблюдов-то за что? Дебилы, бл...\\n      1\n1  Хохлы, это отдушина затюканого россиянина, мол...      1\n2                          Собаке - собачья смерть\\n      1\n3  Страницу обнови, дебил. Это тоже не оскорблени...      1\n4  тебя не убедил 6-страничный пдф в том, что Скр...      1\n5  Для каких стан является эталоном современная с...      1\n6  В шапке были ссылки на инфу по текущему фильму...      0\n7  УПАД Т! ТАМ НЕЛЬЗЯ СТРОИТЬ! ТЕХНОЛОГИЙ НЕТ! РА...      1\n8                      Ебать тебя разносит, шизик.\\n      1\n9                          Обосрался, сиди обтекай\\n      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Собаке - собачья смерть\\n</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Для каких стан является эталоном современная с...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>В шапке были ссылки на инфу по текущему фильму...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>УПАД Т! ТАМ НЕЛЬЗЯ СТРОИТЬ! ТЕХНОЛОГИЙ НЕТ! РА...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Ебать тебя разносит, шизик.\\n</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Обосрался, сиди обтекай\\n</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(os.getcwd() + '\\data')\n",
    "df = pd.read_csv(os.getcwd() + '\\data\\labeled.csv')\n",
    "df['toxic'] = df['toxic'].astype(int)\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import snowball\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# ф-я проверки слова соответствию пунктуации и стоп-словам\n",
    "def clean_stopwords(ls_words=[]):\n",
    "    rus_stem = snowball.SnowballStemmer(language='russian')\n",
    "    morph = MorphAnalyzer()\n",
    "    ls_new = []\n",
    "    if len(ls_words) > 0:\n",
    "        for word in ls_words:\n",
    "            # приводим слово к нормальной форме\n",
    "            morphed = morph.parse(word)[0].normal_form\n",
    "            # stem_word = rus_stem.stem(morphed)\n",
    "            # отбираем слова, которые несут смысловую нагрузку\n",
    "            if morphed not in stopwords.words('russian') and morphed.isalpha():\n",
    "                ls_new.append(rus_stem.stem(morphed))\n",
    "    return ls_new\n",
    "\n",
    "def txt_transform(phrase=''):\n",
    "\n",
    "    # из комментария составляем список слов\n",
    "    ls_tokens = tokenizer.tokenize(phrase)\n",
    "    # возвращаем исходник фразы\n",
    "    return ' '.join(clean_stopwords(ls_tokens))\n",
    "\n",
    "# производим обработку исходных комментариев\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# так как обработку всего DF занимает около часа, то было решено результат выгрузить в csv \n",
    "# и если файл присутствует, то не запускать заново его обработку\n",
    "# если файл отсутствует в папке, то мы его рассчитываем и записываем в csv\n",
    "if not os.path.exists(os.getcwd() + '\\data\\cleaned.csv'):\n",
    "    df['transformed_comments'] = df['comment'].progress_apply(txt_transform)\n",
    "    # подсчитываем слова\n",
    "    df['count_words'] = df['transformed_comments'].apply(lambda x: len(x.split(' ')))\n",
    "    # получаем длину самого длинного комментария\n",
    "    max_length = df['count_words'].sort_values(ascending=False).to_list()[0]\n",
    "    df.to_csv(os.getcwd() + '\\data\\cleaned.csv')\n",
    "# иначе читаем из файла\n",
    "else:\n",
    "    df = pd.read_csv(os.getcwd() + '\\data\\cleaned.csv', index_col=0)\n",
    "    df['transformed_comments'] = df['transformed_comments'].astype(str)\n",
    "    df['comment'] = df['comment'].astype(str)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             comment  toxic  \\\n0               Верблюдов-то за что? Дебилы, бл...\\n      1   \n1  Хохлы, это отдушина затюканого россиянина, мол...      1   \n2                          Собаке - собачья смерть\\n      1   \n3  Страницу обнови, дебил. Это тоже не оскорблени...      1   \n4  тебя не убедил 6-страничный пдф в том, что Скр...      1   \n5  Для каких стан является эталоном современная с...      1   \n6  В шапке были ссылки на инфу по текущему фильму...      0   \n7  УПАД Т! ТАМ НЕЛЬЗЯ СТРОИТЬ! ТЕХНОЛОГИЙ НЕТ! РА...      1   \n8                      Ебать тебя разносит, шизик.\\n      1   \n9                          Обосрался, сиди обтекай\\n      1   \n\n                                transformed_comments  count_words  \n0                                     верблюд деб бл            3  \n1  хохол эт отдушин затюкан россиянин мол вон хох...           13  \n2                                  собак собач смерт            3  \n3  страниц обнов деб эт оскорблен доказа факт деб...           16  \n4  убед страничн пдф скрипал отрав росс анализиро...           10  \n5  стан явля эталон современ систем здравоохранен...           10  \n6  шапк ссылк инф текущ фильм марвест ссылк замен...           27  \n7  упад стро технолог разворова трещин пош л туп ...           12  \n8                                   еба разнос шизик            3  \n9                                обосра сидет обтека            3  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>toxic</th>\n      <th>transformed_comments</th>\n      <th>count_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n      <td>1</td>\n      <td>верблюд деб бл</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n      <td>1</td>\n      <td>хохол эт отдушин затюкан россиянин мол вон хох...</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Собаке - собачья смерть\\n</td>\n      <td>1</td>\n      <td>собак собач смерт</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n      <td>1</td>\n      <td>страниц обнов деб эт оскорблен доказа факт деб...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n      <td>1</td>\n      <td>убед страничн пдф скрипал отрав росс анализиро...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Для каких стан является эталоном современная с...</td>\n      <td>1</td>\n      <td>стан явля эталон современ систем здравоохранен...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>В шапке были ссылки на инфу по текущему фильму...</td>\n      <td>0</td>\n      <td>шапк ссылк инф текущ фильм марвест ссылк замен...</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>УПАД Т! ТАМ НЕЛЬЗЯ СТРОИТЬ! ТЕХНОЛОГИЙ НЕТ! РА...</td>\n      <td>1</td>\n      <td>упад стро технолог разворова трещин пош л туп ...</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Ебать тебя разносит, шизик.\\n</td>\n      <td>1</td>\n      <td>еба разнос шизик</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Обосрался, сиди обтекай\\n</td>\n      <td>1</td>\n      <td>обосра сидет обтека</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ищем дублирующиеся строки, у которых признак токсичности при этом разный\n",
    "ls_different_duplicates_comments = df[df.duplicated(subset=['transformed_comments', 'toxic'])]['transformed_comments']\n",
    "# получаем по таким записям список индексов\n",
    "ls_dif_dup_com_index = df[df['transformed_comments'].isin(ls_different_duplicates_comments)].index.to_list()\n",
    "# удаляем все такие записи, которые вносят неопределенность\n",
    "df = df.drop(index=ls_dif_dup_com_index, axis=0)\n",
    "# из оставшихся записей оставляем только уникальные по трансформированным записям\n",
    "df = df.drop(index=df[df.duplicated(subset=['transformed_comments'])].index.to_list(), axis=0)\n",
    "\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[('эт', 4843),\n ('все', 2415),\n ('моч', 1599),\n ('год', 1489),\n ('котор', 1381),\n ('ве', 1311),\n ('прост', 1275),\n ('ещ', 1257),\n ('сво', 1080),\n ('человек', 1079)]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "from collections import Counter\n",
    "# формируем bag of words\n",
    "bag = Counter(sum(df['transformed_comments'].str.split(' ').to_list(),[]))\n",
    "# преобразуем в список\n",
    "bag = list(bag.items())\n",
    "# производим сортировку по значению из списка 2 (по порядку)\n",
    "bag.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "bag.items()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 147 candidates, totalling 735 fits\n",
      "\n",
      "Подбор параметров осуществлен за: 00:03:03\n",
      "Лучше зачение метрики F1: 0.8167155088084691 наблюдается при следующих параметрах TF-IDF:\n",
      "analyzer='char_wb', max_df=0.9, min_df=20, ngram_range=(1, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "#формируем наборы параметров для подбора\n",
    "min_df = [1] + list(range(0, 31, 5))[1:]\n",
    "max_df = list(np.array(np.arange(70,101, 5))/100)\n",
    "ngram_range = [x for x in combinations_with_replacement(list(range(3))[1:], 2)]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#объявляем TF-IDF и логистическую регрессию\n",
    "logical = LogisticRegression(max_iter=10000, C=3, solver='liblinear')\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb')\n",
    "# создаем pipeline для работы с GridSearchCV\n",
    "pipeline = Pipeline([('vect', vectorizer), ('clf', logical)])\n",
    "params = {'vect__min_df': min_df, 'vect__max_df': max_df, 'vect__ngram_range': ngram_range}\n",
    "\n",
    "grid_search_clf = GridSearchCV(pipeline, params, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "#для расчета времени работы подбора параметров\n",
    "t_start = time.time()\n",
    "#\n",
    "grid_search_clf.fit(df['transformed_comments'], df['toxic'])\n",
    "t_end = time.time()\n",
    "\n",
    "print('')\n",
    "print(f\"Подбор параметров осуществлен за: {time.strftime('%H:%M:%S', time.gmtime(t_end-t_start))}\")\n",
    "print(f\"Лучше зачение метрики F1: {grid_search_clf.best_score_} наблюдается при следующих параметрах TF-IDF:\")\n",
    "print(f\"{str(grid_search_clf.best_estimator_[0]).replace('TfidfVectorizer', '')[1:-1]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат по F1-мере: 0.7182770663562281\n",
      "Результат по метрике accuracy: 0.8262742282842785\n",
      "Результат по метрике recall: 0.6641550053821313\n",
      "Результат по метрике precision: 0.7820025348542459\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      1857\n",
      "           1       0.78      0.66      0.72       929\n",
      "\n",
      "    accuracy                           0.83      2786\n",
      "   macro avg       0.81      0.79      0.80      2786\n",
      "weighted avg       0.82      0.83      0.82      2786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# производим обучение на оптимальных параметрах\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# разбиваем на тренировочную и тестовую выборки\n",
    "df_train, df_test = train_test_split(\n",
    "    df[['transformed_comments','toxic']]\n",
    "    , test_size=0.2\n",
    "    , train_size=0.8\n",
    "    , random_state=42)\n",
    "\n",
    "# объявляем TF-IDF с оптимальными параметрами\n",
    "vectorizer = grid_search_clf.best_estimator_[0]\n",
    "X_train = vectorizer.fit_transform(df_train['transformed_comments'])\n",
    "X_test = vectorizer.transform(df_test['transformed_comments'])\n",
    "y_train = df_train['toxic']\n",
    "y_test = df_test['toxic']\n",
    "\n",
    "# запускаем обучение\n",
    "logical.fit(X_train, y_train)\n",
    "y_pred = logical.predict(X_test)\n",
    "\n",
    "# производим оценки\n",
    "report = classification_report(y_true=y_test, y_pred=y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Результат по F1-мере: {f1}')\n",
    "print(f'Результат по метрике accuracy: {accuracy}')\n",
    "print(f'Результат по метрике recall: {recall}')\n",
    "print(f'Результат по метрике precision: {precision}', end='\\n\\n')\n",
    "\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}